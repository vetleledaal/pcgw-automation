{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'User-Agent': 'VetleBot/0.1'\n",
    "}\n",
    "\n",
    "import re\n",
    "PATTERN_PAGE_LINK = re.compile(r'^(?:<ul>)?<li><a href=\"\\/wiki\\/[^\"]+\" title=\"[^\"]+\">([^<]+)<\\/a><\\/li>', re.MULTILINE)\n",
    "PATTERN_NEXT_PAGE = re.compile(r'<a href=\"([^\"]+)\"[^>]*>Next page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first \"File:\" page\n",
    "import requests\n",
    "r = requests.get('https://pcgamingwiki.com/w/index.php?title=Special%3APrefixIndex&prefix=&namespace=6', headers=headers)\n",
    "done_processing = False\n",
    "all_pages = []\n",
    "\n",
    "import html\n",
    "while done_processing is False:\n",
    "    # Extract pages from prefix page\n",
    "    pages = re.findall(PATTERN_PAGE_LINK, r.text)\n",
    "\n",
    "    # Clean HTML entities\n",
    "    pages = [html.unescape(page) for page in pages]\n",
    "    \n",
    "    # Append pages found to list\n",
    "    all_pages += pages\n",
    "    done_processing = True\n",
    "\n",
    "    # Get the next prefix page, if it exists\n",
    "    m = re.search(PATTERN_NEXT_PAGE, r.text)\n",
    "    if m is not None:\n",
    "        # Clean it\n",
    "        url = html.unescape(m.group(1))\n",
    "        \n",
    "        # Add prefix, if needed\n",
    "        url = 'https://pcgamingwiki.com' + url if url[0] == '/' else url\n",
    "        \n",
    "        # Make request\n",
    "        r = requests.get(url, headers=headers)\n",
    "        done_processing = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49819"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(all_pages, open('../data_pcgw/all_files.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make unique\n",
    "all_pages = list(set(all_pages))\n",
    "all_pages.sort() # for predictability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49819"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_pages)\n",
    "all_images = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997/997\r"
     ]
    }
   ],
   "source": [
    "# for every 50, get size\n",
    "import math\n",
    "batch_size = 50\n",
    "batches = math.ceil(len(all_pages) / batch_size)\n",
    "\n",
    "export_dir = '../data_pcgw'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(export_dir):\n",
    "    os.makedirs(export_dir)\n",
    "\n",
    "import json\n",
    "for i in range(batches):\n",
    "    #if(i <= 324):\n",
    "    #    continue\n",
    "    pages = all_pages[i * batch_size : i * batch_size + batch_size]\n",
    "    pages_comb = '|'.join(('File:' + p for p in pages))\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'titles': pages_comb,\n",
    "        'prop': 'imageinfo',\n",
    "        'iiprop': 'size|url',\n",
    "        'format': 'json',\n",
    "    }\n",
    "    r = requests.get('https://pcgamingwiki.com/w/api.php', headers=headers, params=params)\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    data = json.loads(r.text)\n",
    "    for x in data['query']['pages'].values():\n",
    "        try:\n",
    "            imi = x['imageinfo'][0]\n",
    "            all_images[x['title']] = {'size': imi['size'], 'width': imi['width'], 'height': imi['height'], 'url': imi['url']}\n",
    "        except:\n",
    "            pass # Bad\n",
    "    print(f'{i + 1}/{batches}', end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49817"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(all_images, open('../data_pcgw/imageinfo.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_images = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49817/49817 (smaller: 15930, larger: 3, missing: 324)\r"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "total = len(all_images)\n",
    "for title, info in all_images.items():\n",
    "    #if i < 39621:\n",
    "    #    i += 1\n",
    "    #    continue\n",
    "    \n",
    "    # TODO: handle timeouts\n",
    "    r = requests.head(info['url'], headers=headers)\n",
    "    \n",
    "    actual_size = int(r.headers.get('Content-Length', '-1'))\n",
    "    expected_size = info['size']\n",
    "    \n",
    "    if r.status_code != 200:\n",
    "        bad_images.setdefault('missing', []).append(title)\n",
    "    elif actual_size < expected_size:\n",
    "        bad_images.setdefault('actually_smaller', []).append(title)\n",
    "    elif actual_size > expected_size:\n",
    "        bad_images.setdefault('actually_larger', []).append(title)        \n",
    "    print(f'{i + 1}/{total} (smaller: %s, larger: %s, missing: %s)' % (len(bad_images.get('actually_smaller', [])), len(bad_images.get('actually_larger', [])), len(bad_images.get('missing', []))), end='\\r')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(bad_images, open('../data_pcgw/bad_images.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data_pcgw/images_actually_smaller.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in bad_images['actually_smaller']:\n",
    "        f.write('%s\\n' % item)\n",
    "with open('../data_pcgw/images_actually_larger.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in bad_images['actually_larger']:\n",
    "        f.write('%s\\n' % item)\n",
    "with open('../data_pcgw/images_missing.txt', 'w', encoding='utf-8') as f:\n",
    "    for item in bad_images['missing']:\n",
    "        f.write('%s\\n' % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
